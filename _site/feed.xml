<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.1.6">Jekyll</generator><link href="http://mengdong.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="http://mengdong.github.io/" rel="alternate" type="text/html" /><updated>2016-08-01T00:25:08-07:00</updated><id>http://mengdong.github.io/</id><title>Just Another Blog</title><subtitle>Dong&#39;s personal website</subtitle><author><name>Dong Meng (孟东)</name></author><entry><title>Speed Up Ad-hoc Analytics with SparkSQL, Parquet and Alluxio</title><link href="http://mengdong.github.io/2016/07/15/speed-up-query-with-spark/" rel="alternate" type="text/html" title="Speed Up Ad-hoc Analytics with SparkSQL, Parquet and Alluxio" /><published>2016-07-15T13:15:00-07:00</published><updated>2016-07-15T13:15:00-07:00</updated><id>http://mengdong.github.io/2016/07/15/speed-up-query-with-spark</id><content type="html" xml:base="http://mengdong.github.io/2016/07/15/speed-up-query-with-spark/">&lt;p&gt;In current big data enterprise ecosystems, there are always new choices when it comes to analytics and data science. Apache incubates so many projects that people are always confused how to choose from. Among the pipeline of data science, Ad-hoc query is an important aspect, running different queries will lead to exploratory statistics that help understanding the shape of data. In reality, for many company and practices, Hive is still their working horse. As ancient as Hive is, different groups might hack it in a different way to make it handy to use, still, I heard lots of complaints on the query never be able to finish. Spending time waiting on query execution and adjust query results slowed down the pace of data science discovery.&lt;/p&gt;

&lt;p&gt;Personally, I like using spark to run the ad-hoc queries comparing with Hive map-reduce program, mostly due to the ease of doing other things on spark at the same time. I don&#39;t have to switch back and forth with different tools. Recently, I also looked into &lt;a href=&quot;http://www.alluxio.org/&quot;&gt; Alluxio&lt;/a&gt; which is a distributed in-memory file system. In this article, I will demonstrate examples to use SparkSQL, Parquet and Alluxio to speed up Ad-hoc query analytics. Using Spark to
accelerate query, data locality is the key.&lt;/p&gt;

&lt;h3 class=&quot;section-heading&quot;&gt;Install Alluxio with MapR&lt;/h3&gt;
&lt;p&gt;At first, we begin with an existing MapR 5.1 system running on 3-node AWS instance(m4.2xlarge). We download Alluxio from Github and compile with Mapr5.1 artifacts.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone git://github.com/alluxio/alluxio.git
cd alluxio
git checkout v1.2.0
mvn clean package -Dhadoop.version=2.7.0-mapr-1602 -Pspark -DskipTests
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Oracle Java 8 is used to compile Alluxio, it is also the same Java MapR system is running on. However, to launch Alluxio webUI, it is required to switch back to Java 7 temporally. We also make a few change to configuration, adding alluxio-env.sh: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ALLUXIO_MASTER_HOSTNAME=${ALLUXIO_MASTER_HOSTNAME:-&quot;node1 host name&quot;}
ALLUXIO_WORKER_MEMORY_SIZE=${ALLUXIO_WORKER_MEMORY_SIZE:-&quot;5120MB&quot;}
ALLUXIO_RAM_FOLDER=${ALLUXIO_RAM_FOLDER:-&quot;/mnt/ramdisk&quot;}
ALLUXIO_UNDERFS_ADDRESS=${ALLUXIO_UNDERFS_ADDRESS:-&quot;/mapr/clustername/tmp/underFSStorage&quot;}
ALLUXIO_JAVA_OPTS+=&quot; -Dalluxio.master.journal.folder=/mapr/clustername/tmp/journal&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Those configuration will put under file storage of Alluxio on Mapr File System as will as master journal, also setting 5GB memory for each Alluxio Working. We can even set up a dedicated volume in MapRFS to serve as the under file system for Alluxio. Also add worker file with hostname of 3 node we planned to have Alluxio Working running.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;node1
node2
node3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, on top of our 3-node MapR cluster, we have an Alluxio architecture that master running on node1, and workers running on node1, node2, node3. We just run a few commands to get alluxio running, and you will be able to reach the webUI at node1:19999&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;clush -ac /opt/mapr/alluxio/conf
cd /opt/mapr/alluxio/ 
bin/alluxio format
bin/alluxio-start.sh all
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 class=&quot;section-heading&quot;&gt;Prepare the data&lt;/h3&gt;

&lt;p&gt;For comparison purpose, we also build a 4-node cloudera cluster(m4.2xlarge) with CDH-5.8.0 and put alluxio on its 3 data node with same architecture. We run a standalone spark shell on both cluster, with spark-master on node1, 3 worker each with 10GB memory on node[1-3]. We will use a &lt;a href=&quot;https://www.kaggle.com/c/avazu-ctr-prediction&quot;&gt;click-through-rate prediction&lt;/a&gt; data from Kaggle as sample data we work on, the size is 5.9 GB, contains over 40 million rows. To lauch the spark
shell, we use &lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;spark-shell --master spark://node1:7077 --executor-memory 2G --packages com.databricks:spark-csv_2.1:0:1.4.0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;In spark shell, we load the csv from maprfs and on hdfs in their respected path.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
val trainSchema = StructType(Array(
    StructField(&quot;id&quot;, StringType, false),
    StructField(&quot;click&quot;, IntegerType, true),
    StructField(&quot;hour&quot;, IntegerType, true),
    StructField(&quot;C1&quot;, IntegerType, true),
    StructField(&quot;banner_pos&quot;, IntegerType, true),
    StructField(&quot;site_id&quot;, StringType, true),
    StructField(&quot;site_domain&quot;, StringType, true),
    StructField(&quot;site_category&quot;, StringType, true),
    StructField(&quot;app_id&quot;, StringType, true),
    StructField(&quot;app_domain&quot;, StringType, true),
    StructField(&quot;app_category&quot;, StringType, true),
    StructField(&quot;device_id&quot;, StringType, true),
    StructField(&quot;device_ip&quot;, StringType, true),
    StructField(&quot;device_model&quot;, StringType, true),
    StructField(&quot;device_type&quot;, IntegerType, true),
    StructField(&quot;device_conn_type&quot;, IntegerType, true),
    StructField(&quot;C14&quot;, IntegerType, true),
    StructField(&quot;C15&quot;, IntegerType, true),
    StructField(&quot;C16&quot;, IntegerType, true),
    StructField(&quot;C17&quot;, IntegerType, true),
    StructField(&quot;C18&quot;, IntegerType, true),
    StructField(&quot;C19&quot;, IntegerType, true),
    StructField(&quot;C20&quot;, IntegerType, true),
    StructField(&quot;C21&quot;, IntegerType, true)
))

val train = sqlContext.read.format(&quot;com.databricks.spark.csv&quot;)
    .option(&quot;header&quot;, &quot;true&quot;)
    .schema(trainSchema)
    .load(trainPath)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we write the file three times to generate the data we need: 1, write to alluxio with csv format, 2, write to alluxio with parquet format, 3, write to hdfs/maprfs with parquet format. Since the csv format is already there on hdfs/maprfs.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
train.write.parquet(&quot;maprfs:///tmp/train_parquet&quot;)
train.write.parquet(&quot;alluxio://node1:19998/train_parquet&quot;)
train.write
    .format(&quot;com.databricks.spark.csv&quot;)
    .option(&quot;header&quot;, &quot;true&quot;)
    .save(&quot;alluxio://node:19998/train_crt&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we take a look the file size, we can see that parquet file is more efficient in size, 5.9GB csv data is compressed to less than 1GB&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;shadow&quot; width=&quot;500&quot; src=&quot;/img/alluxio/compressed_parquet.png&quot; /&gt;&lt;/p&gt;

&lt;h3 class=&quot;section-heading&quot;&gt;Run SparkSQL on Hot Data&lt;/h3&gt;

&lt;p&gt;This is how we plan to read the data and monitor different performance. I will show how parquet can increase query performance and when it is useful to use Alluxio. Before we read any files, we will remove the OS cache to make more accurate measurement. &lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;clush -a &quot;sudo sh -c &#39;free &amp;amp;&amp;amp; sync &amp;amp;&amp;amp; echo 3 &amp;gt; /proc/sys/vm/drop_caches &amp;amp;&amp;amp; free&#39;&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;td&gt;Dist\file type&lt;/td&gt;
        &lt;td&gt;csv files&lt;/td&gt;
        &lt;td&gt;parquet files&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Cloudera&lt;/td&gt;
        &lt;td&gt;sc.textFile/dataframe csv reader&lt;/td&gt;
        &lt;td&gt;dataframe parquet reader&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;MapR&lt;/td&gt;
        &lt;td&gt;sc.textFile/dataframe csv reader&lt;/td&gt;
        &lt;td&gt;dataframe parquet reader&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;We can capture the time of execution through sparkUI, but we can also write a small scala snippet to do that: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;val start_time=System.nanoTime()
train.count \\or some other operations
val end_time = System.nanoTime()
println(&quot;Time elapsed: &quot; + (end_time-start_time)/1000000 + &quot; milliseconds&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First, we read csv data into RDD with &lt;code&gt;textFile()&lt;/code&gt; in spark and do a simple count on csv file. Here, one stange thing you might notice is that cached RDD turns out to be slower. I want to emphasize that because RDD is not compressed much when cached into spark, for example, dataframe/dataset are compressed much more effficiently in spark. Hence with our limited memory assigned, we actually can only cache 15% of the data, just a fraction of the whole. So when trying to run query on cached spark RDD, we want to make sure to assign enough executor memory.&lt;/p&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;td&gt;Dist &lt;/td&gt;
        &lt;td&gt;MapR&lt;/td&gt;
        &lt;td&gt;Cloudera&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;textFile&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;300&quot; src=&quot;/img/alluxio/mapr_txt.png&quot; /&gt;&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;300&quot; src=&quot;/img/alluxio/cloudera_txt.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;textFile from Alluxio&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;300&quot; src=&quot;/img/alluxio/mapr_txt_allu.png&quot; /&gt;&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;300&quot; src=&quot;/img/alluxio/cloudera_txt_allu.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;textFile cached&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;300&quot; src=&quot;/img/alluxio/mapr_txt_cache.png&quot; /&gt;&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;300&quot; src=&quot;/img/alluxio/cloudera_txt_cache.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
&lt;table&gt;

&lt;p&gt;Secondly, we use databricks&#39; package to read csv into dataframe in spark and do a simple count on csv file. Here, we notice much better compression and a huge lift when caching the spark dataframe into memory.&lt;/p&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;td&gt;Dist &lt;/td&gt;
        &lt;td&gt;MapR&lt;/td&gt;
        &lt;td&gt;Cloudera&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;csv reader&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;300&quot; src=&quot;/img/alluxio/mapr_csv.png&quot; /&gt;&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;300&quot; src=&quot;/img/alluxio/cloudera_csv.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;csv reader with alluxio&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;300&quot; src=&quot;/img/alluxio/mapr_csv_allu.png&quot; /&gt;&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;300&quot; src=&quot;/img/alluxio/cloudera_csv_allu.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;csv reader cached&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;300&quot; src=&quot;/img/alluxio/mapr_csv_cache.png&quot; /&gt;&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;300&quot; src=&quot;/img/alluxio/cloudera_csv_cache.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
&lt;table&gt;

&lt;p&gt;Lastly, we read parquet into dataframe in spark and do a simple count on parquet file. We can observe that parquet is very efficient for columnar type of query due its great design. Plus, it works very well with Apache Drill.&lt;/p&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;td&gt;Dist &lt;/td&gt;
        &lt;td&gt;MapR&lt;/td&gt;
        &lt;td&gt;Cloudera&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;csv reader&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;300&quot; src=&quot;/img/alluxio/mapr_parquet.png&quot; /&gt;&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;300&quot; src=&quot;/img/alluxio/cloudera_parquet.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;parquet reader with alluxio&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;300&quot; src=&quot;/img/alluxio/mapr_parquet_allu.png&quot; /&gt;&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;300&quot; src=&quot;/img/alluxio/cloudera_parquet_allu.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;parquet reader cached&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;300&quot; src=&quot;/img/alluxio/mapr_parquet_cache.png&quot; /&gt;&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;300&quot; src=&quot;/img/alluxio/cloudera_parquet_cache.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
&lt;table&gt;

&lt;p&gt;We can observe that utilizing cached dataframe and RDD can speed up query greatly. If we look into how the task is executed, we will notice for cached task, all the locality level of the tasks showed &lt;code&gt;&quot;PROCESS_LOCAL&quot;&lt;/code&gt; while for non-cached task, they showed &lt;code&gt;&quot;NODE_LOCAL&quot;&lt;/code&gt;. This is why I would say data locality is key on query speed here and why alluxio would be successful if you have many remote data centers. But you can achieve similiar idea with MapR tech, just create a dedicated volume mirror to some volume with hot data and place it on local clusters.&lt;/p&gt; 
&lt;table&gt;
    &lt;tr&gt;
        &lt;td&gt;cached&lt;/td&gt;
        &lt;td&gt;non-cached&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;400&quot; src=&quot;/img/alluxio/process_local.png&quot; /&gt;&lt;/td&gt;
        &lt;td&gt;&lt;img class=&quot;shadow&quot; width=&quot;400&quot; src=&quot;/img/alluxio/node_local.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
&lt;table&gt; 

&lt;h3 class=&quot;section-heading&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;To summarize, if we want to accelerate the query speed on hadoop. We should really use the cached sparkSQL, and try to use parquet format for the right use case. Alluxio is great if you have remote data centers or hetergenous storage layer, it can provide the data locality required for spark execution. And the benefits comes in as resilence against job failure and share between multiple spark sessions. To truely monitor the system performance, we should monitor file system throughput stats. This is just a rough repesentation of the performance metrics. We also observe that the larger underneath data is, we gain more benefits using Alluxio or cache them in memory.&lt;/p&gt; 

&lt;p&gt;Also, if you interested in using Drill to query Alluxio, just put the compiled alluxio jar file &lt;code&gt;alluxio-core-client-1.2.0-jar-with-dependencies.jar&lt;/code&gt; under &lt;code&gt;jars/lassb&lt;/code&gt;. And add following lines to &lt;code&gt;conf/core-site.xml&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;fs.alluxio.impl&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;alluxio.hadoop.FileSystem&amp;lt;value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Have fun query the data!&lt;/p&gt;

&lt;/table&gt;&lt;/table&gt;&lt;/table&gt;&lt;/table&gt;&lt;/table&gt;&lt;/table&gt;&lt;/table&gt;&lt;/table&gt;</content><author><name>Dong Meng</name></author><category term="Open Source" /><category term="Big Data" /><category term="Apache Spark" /><category term="Hadoop" /><summary>In current big data enterprise ecosystems, there are always new choices when it comes to analytics and data science. Apache incubates so many projects that people are always confused how to choose from. Among the pipeline of data science, Ad-hoc query is an important aspect, running different queries will lead to exploratory statistics that help understanding the shape of data. In reality, for many company and practices, Hive is still their working horse. As ancient as Hive is, different groups might hack it in a different way to make it handy to use, still, I heard lots of complaints on the query never be able to finish. Spending time waiting on query execution and adjust query results slowed down the pace of data science discovery.</summary></entry><entry><title>My First Blog</title><link href="http://mengdong.github.io/2016/06/25/my-first-blog/" rel="alternate" type="text/html" title="My First Blog" /><published>2016-06-25T08:16:00-07:00</published><updated>2016-06-25T08:16:00-07:00</updated><id>http://mengdong.github.io/2016/06/25/my-first-blog</id><content type="html" xml:base="http://mengdong.github.io/2016/06/25/my-first-blog/">&lt;p&gt;This is my first blog. Try out a few things here&lt;/p&gt;

&lt;p&gt;I am currently a data scientist at &lt;a href=&quot;http://www.mapr.com/&quot;&gt;MapR Techologies&lt;/a&gt;. I will be posting on my company&#39;s website as well, &lt;a href=&quot;https://www.mapr.com/blog/author/dong-meng&quot;&gt;my blogs on MapR&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dong Meng</name></author><category term="Open Source" /><category term="Machine Learning" /><summary>This is my first blog. Try out a few things here</summary></entry></feed>
